{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Questions:\n",
        "\n",
        "Que.1 What is a parameter?\n",
        "\n",
        "\n",
        "Ans. parameters are defined as the internal variables of this model. They are learned or estimated purely from the data during training as every ML algorithm has mechanisms to optimize these parameters.\n",
        "\n",
        "Training typically starts with parameters being initialized to some values. As training progresses, the initial values are updated using an optimization algorithm (e.g. gradient descent). The learning algorithm is continuously updating the parameter values as learning progress. At the end of the learning process, model parameters are what constitute the model itself.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Que.2 What is correlation?\n",
        "\n",
        "\n",
        "\n",
        "Ans.Correlation explains how one or more variables are related to each other. These variables can be input data features which have been used to forecast our target variable.\n",
        "\n",
        "Correlation, statistical technique which determines how one variables moves/changes in relation with the other variable. It gives us the idea about the degree of the relationship of the two variables. It’s a bi-variate analysis measure which describes the association between different variables. In most of the business it’s useful to express one subject in terms of its relationship with others.\n",
        "\n",
        "\n",
        "What does negative correlation mean?\n",
        "\n",
        "Negative Correlation: Two features (variables) can be negatively correlated with each other. It means that when the value of one variable increase then the value of the other variable(s) decreases.\n",
        "\n",
        "Que.3 define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "\n",
        "Ans. Machine learning is a branch of artificial intelligence that enables algorithms to uncover hidden patterns within datasets. It allows them to predict new, similar data without explicit programming for each task. Machine learning finds applications in diverse fields such as image and speech recognition, natural language processing, recommendation systems, fraud detection, portfolio optimization, and automating tasks.\n",
        "\n",
        "\n",
        "Components of machine learning:\n",
        "\n",
        "There are tens of thousands of machine learning algorithms and hundreds of new algorithms are developed every year.\n",
        "\n",
        "Every machine learning algorithm has three components:\n",
        "\n",
        "* Representation:\n",
        " This implies how to represent knowledge. Examples include decision trees, sets of rules, instances, graphical models, neural networks, support vector machines, model ensembles and others.\n",
        "\n",
        "* Evaluation:\n",
        "This is the way to evaluate candidate programs (hypotheses). Examples include accuracy, prediction and recall, squared error, likelihood, posterior probability, cost, margin, entropy k-L divergence and others.\n",
        "\n",
        "* Optimization:\n",
        " Last but not the least, optimization is the way candidate programs are generated and is known as the search process. For example, combinatorial optimization, convex optimization, and constrained optimization.\n",
        "\n",
        "All machine learning algorithms are a combination of these three components and a framework for understanding all algorithms.\n",
        "\n",
        "\n",
        "Que.4 How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "\n",
        "Ans.Loss is a value that represents the summation of errors in our model. It measures how well (or bad) our model is doing. If the errors are high, the loss will be high, which means that the model does not do a good job. Otherwise, the lower it is, the better our model works.\n",
        "\n",
        "To calculate the loss, a loss or cost function is used. There are several different cost functions to use. Each penalizes errors in different ways, and the problem determines which one is better to use. Cross-Entropy and Mean Squared Error are the most commonly used for classification and regression problems, respectively.\n",
        "\n",
        "\n",
        "Que.5  What are continuous and categorical variables\n",
        "\n",
        "Ans. In statistical research, variables are attributes or characteristics that can take on different values. Understanding the type of variable you are working with is crucial for selecting appropriate statistical tests and interpreting results accurately\n",
        "\n",
        "Continuous Variables:\n",
        "\n",
        "Continuous variables are quantitative variables that can take any value within a range. They are measured rather than counted and can have an infinite number of possible values between any two points. Examples include height, weight, temperature, and time\n",
        " Continuous variables are often visualized using histograms, box plots, or scatter plots and are analyzed using methods such as mean, median, normal distributions, and regression analysis.\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "Categorical variables represent groupings or categories and can be further divided into binary, nominal, and ordinal variables\n",
        " They are often recorded as numbers, but these numbers represent categories rather than actual amounts.\n",
        "\n",
        "* Binary Variables: These have two categories, such as yes/no or win/lose.\n",
        "\n",
        "* Nominal Variables: These have multiple categories without any order, such as species names or colors.\n",
        "\n",
        "* Ordinal Variables: These have multiple categories with a specific order, such as finishing places in a race or rating scales\n",
        "\n",
        "\n",
        "Que.6 How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "\n",
        "Ans. Techniques to perform Categorical Data Encoding\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "Label Encoding assigns a unique integer to each category. However, it does not respect the order of the categories, making it more suitable for nominal data where the order doesn’t matter. For ordinal data, Ordinal Encoding is a better choice as it preserves the ordinal relationship.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "\n",
        "One-Hot Encoding converts categorical data into a binary format, where each category is represented by a new column. A value of 1 indicates the presence of that category, while 0 indicates its absence. This technique is ideal for nominal data (categories without an order), preventing the model from assuming any relationship between the categories.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "\n",
        "Ordinal Encoding is used for ordinal data, where categories have a natural order. It converts categorical values into numeric values, preserving the inherent order.\n",
        "\n",
        "4. Target Encoding\n",
        "\n",
        "Target Encoding (also known as Mean Encoding) is a technique where each category in a feature is replaced by the mean of the target variable for that category. This technique is especially useful when there is a relationship between the categorical feature and the target variable.\n",
        "\n",
        "5. Binary Encoding\n",
        "\n",
        "Binary encoding is a more compact version of one-hot encoding. Each category is assigned a unique binary code. The binary code is then split into multiple columns. This method is suitable for datasets with high cardinality (many unique categories), as it results in fewer columns compared to one-hot encoding.\n",
        "\n",
        "6. Frequency Encoding\n",
        "\n",
        "Frequency Encoding assigns each category a value based on its frequency in the dataset. This technique can be useful for handling high-cardinality categorical features (features with many unique categories).\n",
        "\n",
        "\n",
        "Que.7 What do you mean by training and testing a dataset\n",
        "\n",
        "Ans.There are two key types of data used for machine learning training and testing data. They each have a specific function to perform when building and evaluating machine learning models. Machine learning algorithms are used to learn from data in datasets. They discover patterns and gain knowledge. make choices, and examine those decisions.\n",
        "\n",
        "\n",
        "Testing data is used to determine the performance of the trained model, whereas training data is used to train the machine learning model. Training data is the power that supplies the model in machine learning, it is larger than testing data. Because more data helps to more effective predictive models. When a machine learning algorithm receives data from our records, it recognizes patterns and creates a decision-making model.\n",
        "\n",
        "You will need unknown information to test your machine learning model after it was created (using your training data). This data is known as testing data, and it may be used to assess the progress and efficiency of your algorithms' training as well as to modify or optimize them for better results.\n",
        "\n",
        "* Showing the original set of data.\n",
        "* Be large enough to produce reliable projections\n",
        "\n",
        "Que.8 What is sklearn.preprocessing?\n",
        "\n",
        "Ans.Preprocessing is a crucial step in the machine learning pipeline, as it transforms raw data into a format that is more suitable for modeling. The sklearn.preprocessing module in Scikit-Learn provides several utility functions and transformer classes to facilitate this process.\n",
        "\n",
        "\n",
        "Que 9 What is a Test set?\n",
        "\n",
        "Ans. A test set is a crucial component in the field of statistics, data analysis, and data science, serving as a subset of data used to evaluate the performance of a predictive model. In the context of machine learning, the test set is distinct from both the training set and the validation set. While the training set is utilized to train the model, allowing it to learn the underlying patterns and relationships within the data, the test set is reserved exclusively for assessing how well the model can generalize to unseen data. This separation is vital to ensure that the model’s performance metrics are not biased by the data it was trained on.\n",
        "\n",
        "\n",
        "Que 10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans.We need to split a dataset into train and test sets to evaluate how well our machine learning model performs. The train set is used to fit the model, and the statistics of the train set are known. The second set is called the test data set, this set is solely used for predictions.\n",
        "\n",
        "Scikit-learn alias sklearn is the most useful and robust library for machine learning in Python. The scikit-learn library provides us with the model_selection module in which we have the splitter function train_test_split().\n",
        "\n",
        "Syntax:\n",
        "\n",
        "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "\n",
        "\n",
        "Splitting facts for system mastering models is an crucial step within the version improvement process. It includes dividing the to be had dataset into separate subsets for education, validation, and trying out the version. Here are a few common processes for splitting data:\n",
        "\n",
        "1. Train-Test Split: The dataset is divided right into a training set and a trying out set. The education set is used to educate the model, even as                     the checking out set is used to assess the model’s overall performance. The regular cut up is 70-eighty% for training and 20-30% for                   checking out, but this may vary depending on the scale of the dataset and the precise use case.\n",
        "\n",
        "2. Train-Validation-Test Split: The dataset is split into three subsets – a schooling set, a validation set, and a trying out set. The training set is                  used to train the version, the validation set is used to tune hyperparameters and validate the version’s overall performance for the                        duration of training, and the testing set is used to evaluate the very last version’s overall performance.\n",
        "\n",
        "3. K-fold Cross Validation: The dataset is divided into ok equally sized folds, and the version is educated and evaluated okay instances. Each               time, k-1 folds are used for training, and 1 fold is used for validation/testing. This allows in acquiring greater strong overall performance              estimates and reduces the variance in version evaluation.\n",
        "\n",
        "4. Stratified Sampling: This technique guarantees that the distribution of training or other essential features is preserved in the training and                  trying out units. This is in particular beneficial when coping with imbalanced datasets, wherein some classes may additionally have only a            few    samples.\n",
        "\n",
        "5. Time-primarily based Split: When coping with time collection facts, consisting of stock costs or weather statistics, the dataset is regularly cut            up into schooling and checking out sets based on a chronological order. This facilitates in comparing the model’s performance on future unseen facts\n",
        "\n",
        "\n",
        "\n",
        "Que 11 Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans. Before fitting any model, it is often important to conduct an exploratory data analysis (EDA) in order to check assumptions, inspect the data for anomalies (such as missing, duplicated, or mis-coded data), and inform feature selection/transformation. we will use pandas to explore some of the EDA techniques that are generally employed prior to fitting a regression model.\n",
        "\n",
        "\n",
        "Que.12 What is correlation\n",
        "\n",
        "Ans. Correlation explains how one or more variables are related to each other. These variables can be input data features which have been used to forecast our target variable.\n",
        "\n",
        "Correlation, statistical technique which determines how one variables moves/changes in relation with the other variable. It gives us the idea about the degree of the relationship of the two variables. It’s a bi-variate analysis measure which describes the association between different variables. In most of the business it’s useful to express one subject in terms of its relationship with others.\n",
        "\n",
        "\n",
        "Que 13 What does negative correlation mean?\n",
        "\n",
        "Ans. Negative Correlation: Two features (variables) can be negatively correlated with each other. It means that when the value of one variable increase then the value of the other variable(s) decreases.\n",
        "\n",
        "\n",
        "Qe.14 How can you find correlation between variables in Python?\n",
        "\n",
        "\n",
        "Ans.Correlation summarizes the strength and direction of the linear (straight-line) association between two quantitative variables. Denoted by r, it takes values between -1 and +1. A positive value for r indicates a positive association, and a negative value for r indicates a negative association. The closer r is to 1 the closer the data points fall to a straight line, thus, the linear association is stronger. The closer r is to 0, making the linear association weaker.\n",
        "\n",
        "\n",
        "Que.15 What is causation? Explain difference between correlation and causation with an example\n",
        "\n",
        "\n",
        "Ans.Causation is indicating that X and Y have a cause-and-effect connection with one another. It tells X causes Y. Causation is also understood as a basis. Firstly, causation indicates that two possibilities occur at the same time or one after the other. And secondly, it tells these two variables not only occur jointly, the presence of one drives the other to display.\n",
        "\n",
        "Difference between correlation and causation\n",
        "\n",
        "Comprehending the difference between correlation and causation can make a huge difference – particularly when someone is establishing a decision on something that may be wrong. Say, someone is wondering whether the previous month’s in monthly vibrant users has been generated by the current App Store optimization actions, it makes reason to try this in directive to say for sure whether it’s a correlation or causation.\n",
        "\n",
        "Correlation represents an alliance between variables: when one variable is different, so do the other. A correlation is a statistical hand of the connection between variables. These variables vary jointly: they covary. But this covariation isn’t necessarily due to an immediate or avoiding causal connection.\n",
        "\n",
        "Causation means that changes in one variable bring about changes in the other; there is a cause-and-effect relationship between variables. The two variables are associated with each other and there is also a causal connection between them. A correlation doesn’t indicate causation, but causation always indicates correlation.\n",
        "\n",
        "\n",
        "Que.16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans. An optimizer in Deep Learning is a method or algorithm used to update the weights and biases of a neural network during training. The primary goal of an optimizer is to minimize the loss function, which quantifies how well the model’s predictions match the actual outcomes.\n",
        "\n",
        "By iteratively adjusting the model parameters based on computed gradients, optimizers facilitate the learning process.\n",
        "\n",
        "There are several optimizers used in Deep Learning, each with its unique characteristics and applications. Below are some of the most commonly used optimizers:\n",
        "\n",
        "Gradient Descent:\n",
        "\n",
        "Gradient Descent is the most basic optimization algorithm used in Machine Learning. It updates the parameters by moving them in the direction of the negative gradient of the loss function with respect to those parameters.\n",
        "\n",
        "Example: In a simple linear regression problem, Gradient Descent can be used to find the optimal slope and intercept that minimize prediction errors.\n",
        "\n",
        "\n",
        "1. SGD (Stochastic Gradient Descent):\n",
        "\n",
        "Stochastic Gradient Descent (SGD) updates the model parameters using the gradient of the loss function with respect to the weights. It is efficient, but can be slow, especially in complex models, due to noisy gradients and small updates.\n",
        "\n",
        "2. Adam (Adaptive Moment Estimation):\n",
        "\n",
        "Adam combines the advantages of two other extensions of SGD: AdaGrad and RMSProp.\n",
        "\n",
        "It computes adaptive learning rates for each parameter by considering both first and second moments of the gradients. Adam is one of the most popular optimizers due to its efficient handling of sparse gradients and non-stationary objectives.\n",
        "\n",
        "3. RMSprop (Root Mean Square Propagation):\n",
        "\n",
        "RMSprop is an adaptive learning rate method, that divides the learning rate by an exponentially decaying average of squared gradients. This optimizer is effective for handling non-stationary objectives and is often used for training RNNs.\n",
        "\n",
        "4. Adagrad\n",
        "Adagrad adapts the learning rate to the parameters by scaling it inversely with respect to the square root of the sum of all historical squared gradients. This helps in improving performance for sparse data. However, the learning rate tends to shrink too much over time, causing the optimizer to stop making updates.\n",
        "\n",
        "5. Adadelta\n",
        "Adadelta is an extension of Adagrad. It addresses the problem of excessively diminishing learning rates. It uses a moving window of gradient updates, helping the model learn effectively even with sparse data.\n",
        "\n",
        "6. FTRL (Follow The Regularized Leader)\n",
        "FTRL is an optimization algorithm particularly suited for problems with sparse data, such as those found in large-scale linear models. It maintains two accumulators to track gradients and updates them efficiently.\n",
        "\n",
        "7. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
        "\n",
        "Nadam combines Adam and Nesterov accelerated gradient. It calculates gradients using momentum and adapts the learning rate for each parameter, with an additional Nesterov momentum term.\n",
        "\n",
        "\n",
        "Que 17 What is sklearn.linear_model ?\n",
        "\n",
        "Ans.\n",
        "\n",
        "linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models.\n",
        "\n",
        "The term linear model implies that the model is specified as a linear combination of features. Based on training data, the learning process computes one weight for each feature to form a model that can predict or estimate the target value.\n",
        "\n",
        "Que.18 What does model.fit() do? What arguments must be given\n",
        "\n",
        "Ans. The fit() method in Scikit-Learn is used to train a machine learning model. Training a model involves feeding it with data so it can learn the underlying patterns. This method adjusts the parameters of the model based on the provided data.\n",
        "\n",
        "Syntax\n",
        "The basic syntax for the fit() method is:\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "Steps Involved in Model Training:\n",
        "\n",
        "* Initialization: When a model object is created, its parameters are initialized.\n",
        "\n",
        "* Training: The fit() method adjusts the model parameters based on the input data (X) and the target values (y).\n",
        "\n",
        "* Optimization: The model tries to minimize the error between its predictions and the actual target values.\n",
        "\n",
        "\n",
        "Que.19 what does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans. The model.predict function is commonly used in machine learning frameworks like TensorFlow and Keras to generate predictions based on a trained model. Here's a brief overview of what it does and the arguments it typically requires:\n",
        "\n",
        "What model.predict Does\n",
        "\n",
        "The model.predict function takes input data and returns the model's predictions for that data. These predictions can be probabilities, class labels, or other outputs depending on the type of model and the problem it is solving (e.g., regression, classification).\n",
        "\n",
        "Common Arguments for model.predict:\n",
        "\n",
        "x: The input data. This can be a NumPy array, a TensorFlow tensor, or a dataset object. The shape and type of this data should match what the model expects based on how it was trained.\n",
        "\n",
        "batch_size (optional): The number of samples per batch of computation. If not specified, the default is usually determined by the framework.\n",
        "\n",
        "verbose (optional): Verbosity mode. 0 = silent, 1 = progress bar.\n",
        "\n",
        "steps (optional): Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None.\n",
        "\n",
        "callbacks (optional): List of callback instances to apply during prediction.\n",
        "\n",
        "max_queue_size (optional): Maximum size for the generator queue. If unspecified, it defaults to 10.\n",
        "\n",
        "workers (optional): Number of workers to use for data loading. If unspecified, it defaults to 1.\n",
        "\n",
        "use_multiprocessing (optional): Whether to use process-based threading. If unspecified, it defaults to False.\n",
        "\n",
        "Example Usage\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = load_model('my_model.h5')\n",
        "\n",
        "# Prepare input data\n",
        "input_data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(input_data, batch_size=2, verbose=1)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "\n",
        "Que.20 What are continuous and categorical variables?\n",
        "\n",
        "Ans.Continuous Variables:\n",
        "\n",
        "Continuous variables are quantitative variables that can take any value within a range. They are measured rather than counted and can have an infinite number of possible values between any two points. Examples include height, weight, temperature, and time\n",
        " Continuous variables are often visualized using histograms, box plots, or scatter plots and are analyzed using methods such as mean, median, normal distributions, and regression analysis.\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "Categorical variables represent groupings or categories and can be further divided into binary, nominal, and ordinal variables\n",
        " They are often recorded as numbers, but these numbers represent categories rather than actual amounts.\n",
        "\n",
        "* Binary Variables: These have two categories, such as yes/no or win/lose.\n",
        "\n",
        "* Nominal Variables: These have multiple categories without any order, such as species names or colors.\n",
        "\n",
        "* Ordinal Variables: These have multiple categories with a specific order, such as finishing places in a race or rating scales\n",
        "\n",
        "\n",
        "\n",
        "Que.21 What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "\n",
        "Ans. Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing.\n",
        "\n",
        "Why Use Feature Scaling?\n",
        "\n",
        "Feature scaling is crucial in machine learning for several reasons. Properly scaled data ensures that machine learning algorithms can perform efficiently and accurately without being biased by the range or units of different features. Here are some of the main reasons to use feature scaling:\n",
        "\n",
        "1. Improve Algorithm Performance\n",
        "Many machine learning algorithms work best when features are on a similar scale. Scaling helps models converge faster and find optimal solutions more easily. For example, in gradient descent-based algorithms, feature scaling ensures that the model updates the parameters uniformly, preventing slow convergence due to one feature dominating the learning process.\n",
        "\n",
        "2. Optimize Distance-Based Algorithms\n",
        "Algorithms that rely on distance calculations, such as k-Nearest Neighbors (k-NN), K-Means Clustering, and Support Vector Machines (SVMs), are heavily affected by the scale of features. If one feature has a much larger range than others, it will disproportionately influence the distance calculations, leading to poor clustering or classification results. Scaling ensures that all features contribute equally to the distance measure.\n",
        "\n",
        "3. Prevent Bias from Dominant Features\n",
        "In datasets with features of varying magnitudes (e.g., age vs. income), features with larger ranges can dominate the learning process, making it difficult for the model to learn from smaller features. Feature scaling helps prevent this by ensuring that no single feature disproportionately influences the model.\n",
        "\n",
        "4. Allow Models to Treat All Features Equally\n",
        "Many algorithms assume that the input features have been scaled to a similar range. Without scaling, the model may assign more importance to certain features due to their range, even if they are not necessarily more important. Feature scaling helps models weigh each feature more equally, improving overall performance.\n",
        "\n",
        "5. Make Models More Interpretable\n",
        "When features are on a common scale, it becomes easier to interpret the model’s results and understand the relative importance of each feature. This is particularly useful in linear models like linear regression, where the coefficients can be interpreted as the influence of each feature on the target variable.\n",
        "\n",
        "\n",
        "Que.22 How do we perform scaling in Python?\n",
        "\n",
        "Ans.Scaling in Python is a common preprocessing step in data analysis and machine learning to normalize the range of independent variables or features of data. Here are three popular methods to perform scaling using the scikit-learn library:\n",
        "\n",
        "1. Standardization (StandardScaler)\n",
        "Standardization scales the data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "2. Min-Max Scaling (MinMaxScaler)\n",
        "Min-Max Scaling scales the data to a fixed range, usually 0 to 1.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "3. Robust Scaling (RobustScaler)\n",
        "Robust Scaling uses the median and the interquartile range, making it robust to outliers.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Sample data\n",
        "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "\n",
        "Que.23 what is sklearn.preprocessing?\n",
        "\n",
        "Ans..Preprocessing is a crucial step in the machine learning pipeline, as it transforms raw data into a format that is more suitable for modeling. The sklearn.preprocessing module in Scikit-Learn provides several utility functions and transformer classes to facilitate this process.\n",
        "\n",
        "\n",
        "Que.24 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans.Splitting data into training and testing sets is a crucial step in building a machine learning model. In Python, you can use the train_test_split function from the scikit-learn library to achieve this.\n",
        "\n",
        "Example using train_test_split from scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "    'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the results\n",
        "print(\"X_train:\\n\", X_train)\n",
        "print(\"X_test:\\n\", X_test)\n",
        "print(\"y_train:\\n\", y_train)\n",
        "print(\"y_test:\\n\", y_test)\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* Import Libraries: Import train_test_split from sklearn.model_selection and pandas for data manipulation.\n",
        "\n",
        "* Sample Data: Create a sample dataset using a dictionary and convert it to a DataFrame.\n",
        "\n",
        "* Features and Target: Separate the features (X) and the target variable (y).\n",
        "\n",
        "* Split the Data: Use train_test_split to split the data into training and testing sets. The test_size parameter specifies the proportion of the dataset to include in the test split (e.g., 0.2 for 20%). The random_state parameter ensures reproducibility.\n",
        "\n",
        "This method is straightforward and widely used for preparing data for machine learning models. If you have any specific requirements or need further customization, feel free to let me know!\n",
        "\n",
        "\n",
        "Que.25 Explain data encoding?\n",
        "\n",
        "Ans.Encoding is the process of using various patterns of voltage or current levels to represent 1s and 0s of the digital signals on the transmission link.\n",
        "\n",
        "The common types of line encoding are Unipolar, Polar, Bipolar, and Manchester.\n",
        "\n",
        "Encoding Techniques:\n",
        "The data encoding technique is divided into the following types, depending upon the type of data conversion.\n",
        "\n",
        "* Analog data to Analog signals − The modulation techniques such as Amplitude Modulation, Frequency Modulation and Phase Modulation of analog signals, fall under this category.\n",
        "\n",
        "* Analog data to Digital signals − This process can be termed as digitization, which is done by Pulse Code Modulation (PCM). Hence, it is nothing but digital modulation. As we have already discussed, sampling and quantization are the important factors in this. Delta Modulation gives a better output than PCM.\n",
        "\n",
        "* Digital data to Analog signals − The modulation techniques such as Amplitude Shift Keying (ASK), Frequency Shift Keying (FSK), Phase Shift Keying (PSK), etc., fall under this category. These will be discussed in subsequent chapters.\n",
        "\n",
        "* Digital data to Digital signals − These are in this section. There are several ways to map digital data to digital signals."
      ],
      "metadata": {
        "id": "_nvs6IP4ZgfH"
      }
    }
  ]
}